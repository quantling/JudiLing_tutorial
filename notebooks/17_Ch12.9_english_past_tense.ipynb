{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a5f404",
   "metadata": {},
   "source": [
    "# Chapter 12.9: English past tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c98510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using JudiLing\n",
    "using DataFrames, Statistics, CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f06ea3-d089-490c-b4ff-396e531a811d",
   "metadata": {},
   "source": [
    "**Note**: This notebook was run with Julia 1.11. Different Julia versions may lead to different random word selections and therefore, not all of the following code may run when using a different Julia version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374adb6e",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "First, we have to load the English dataset. We only want to have past tense forms in the heldout (validation) data, but because there is no function for that specific purpose available, we will first use the careful split function to make sure that the validation data only contains words whose lexeme, aspect, tense, person and number as well as all trigrams have already occurred in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419dd0ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train, data_val =\n",
    "JudiLing.loading_data_careful_split(\n",
    "\"../dat/english.csv\", \"english\", \"../dat/careful\",\n",
    "[\"Lexeme\", \"Continuous\", \"Tense\", \"Person\", \"Number\"],\n",
    "n_grams_target_col = \"Word\",\n",
    "grams = 3,\n",
    "val_sample_size = 300,\n",
    "random_seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba3083",
   "metadata": {},
   "source": [
    "Then, we subset the validation data to only contain past tense words, and put the rest back into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b4c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = vcat(data_train, data_val[data_val.Tense .!= \"past\",:])\n",
    "data_val = data_val[data_val.Tense .== \"past\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998eb67",
   "metadata": {},
   "source": [
    "Now we can inspect how many regular and irregular verbs there are in the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d13e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combine(groupby(data_val, :Regularity), nrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2124f",
   "metadata": {},
   "source": [
    "The last preparation step is to create semantic matrices for the training and validation data. We load them from fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a4e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_small, val_small, S_train, S_val = JudiLing.load_S_matrix_from_fasttext(data_train, data_val, :en, target_col=:Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c19eca",
   "metadata": {},
   "source": [
    "# Simulation 1: meaning-form mapping\n",
    "\n",
    "In the first simulation, we use the embeddings of the heldout past tense words to predict their forms. First, we require cue matrices for both the training and the heldout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c53a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cue_obj_train, cue_obj_val = JudiLing.make_cue_matrix(train_small, val_small, \n",
    "    grams=3, target_col=\"Word\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756ae6e",
   "metadata": {},
   "source": [
    "Next, we train an F matrix on the training data, and calculate the Shat matrix to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21ad57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F = JudiLing.make_transform_matrix(cue_obj_train.C, S_train)\n",
    "Shat = cue_obj_train.C * F;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e99cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_SC(Shat, S_train, train_small, :Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f621e",
   "metadata": {},
   "source": [
    "Same for the production matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a157d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = JudiLing.make_transform_matrix(S_train, cue_obj_train.C);\n",
    "Chat_train = S_train * G;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "JudiLing.eval_SC(Chat_train, cue_obj_train.C, train_small, :Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab4502",
   "metadata": {},
   "source": [
    "Run learn paths on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a8f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_learn_train= JudiLing.learn_paths(\n",
    "train_small,\n",
    "cue_obj_train,\n",
    "S_train,\n",
    "F,\n",
    "Chat_train,\n",
    "threshold = 0.01,\n",
    "verbose = true,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3d7d6",
   "metadata": {},
   "source": [
    "Accuracy on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f32e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_acc(res_learn_train, cue_obj_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7f3d5",
   "metadata": {},
   "source": [
    "Accuracy @10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d90d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "JudiLing.eval_acc_loose(res_learn_train, cue_obj_train.gold_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136e54c",
   "metadata": {},
   "source": [
    "Create dataframe with all productions, and join it with the original training dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db63b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = JudiLing.write2df(res_learn_train, train_small, cue_obj_train, cue_obj_train, target_col=:Word)\n",
    "df_train = leftjoin(df_train, train_small, on = :identifier => :Word)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8821f06",
   "metadata": {},
   "source": [
    "Subset to only contain rows with past tense targets, and only the first candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d1bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "past_tense_cands = df_train[(df_train.isbest .== true) .& (df_train.Tense .== \"past\"),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b23cc5",
   "metadata": {},
   "source": [
    "Accuracy for regulars and irregulars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(groupby(past_tense_cands, :Regularity), :iscorrect => mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758175ee",
   "metadata": {},
   "source": [
    "Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ec715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "past_tense_cands[past_tense_cands.iscorrect .== false,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21521f-aaa8-4193-a489-ffc979c12ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"../res/past_tense_errors.csv\", past_tense_cands[past_tense_cands.iscorrect .== false,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223d2e73",
   "metadata": {},
   "source": [
    "- no change: 5\n",
    "- semantic: 39\n",
    "- tense: 2\n",
    "- overregularisation: 8\n",
    "- overirregularisation: 4\n",
    "- other: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98478dc0",
   "metadata": {},
   "source": [
    "Production of heldout forms.\n",
    "First compute Chat matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b8f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Chat_val = S_val * G;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92436600",
   "metadata": {},
   "source": [
    "Then run learn paths algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3878e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_learn_val = JudiLing.learn_paths(\n",
    "train_small,\n",
    "val_small,\n",
    "cue_obj_train.C,\n",
    "S_val,\n",
    "F,\n",
    "Chat_val,\n",
    "cue_obj_train.A,\n",
    "cue_obj_train.i2f,\n",
    "cue_obj_train.f2i, # api changed in 0.3.1\n",
    "max_t = JudiLing.cal_max_timestep(val_small, :Word),\n",
    "max_can = 10,\n",
    "grams = 3,\n",
    "threshold = 0.01,\n",
    "is_tolerant=true,\n",
    "max_tolerance=1,\n",
    "tolerance=-1.,\n",
    "target_col = :Word,\n",
    "verbose = true,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80045ab0",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c98f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "JudiLing.eval_acc(res_learn_val, cue_obj_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205dd7c4",
   "metadata": {},
   "source": [
    "Accuracy @10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045424b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_acc_loose(res_learn_val, cue_obj_val.gold_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fefbb45",
   "metadata": {},
   "source": [
    "# Simulation 2: form-meaning-form mapping\n",
    "\n",
    "For the second simulation, we will first map the base forms of the heldout past tense forms to predict their semantics, then add a past tense vector, and then use these created semantic vectors to predict past tense forms.\n",
    "\n",
    "Load the base forms of the past tense forms with their phonology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06b90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base = JudiLing.load_dataset(\"../dat/english_heldout_base_orth2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283666b",
   "metadata": {},
   "source": [
    "Now we require three cue matrices: one for the training data, one for the base forms, and one for the heldout forms (for verifying the produced forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931f1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cue_obj_train, cue_obj_base = JudiLing.make_combined_cue_matrix(train_small[:, [\"Word\"]], base[:, [\"Word\"]], \n",
    " grams=3, target_col=\"Word\")\n",
    "\n",
    "cue_obj_val = JudiLing.make_cue_matrix(val_small[:, [\"Word\"]], cue_obj_train,\n",
    " grams=3, target_col=\"Word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf32b",
   "metadata": {},
   "source": [
    "Train the F matrix and predict semantic vectors for the training and the base forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036815ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F = JudiLing.make_transform_matrix(cue_obj_train.C, S_train)\n",
    "Shat_train = cue_obj_train.C * F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b6d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_SC(Shat_train, S_train, train_small, :Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ecc58",
   "metadata": {},
   "source": [
    "Now we create semantic vectors for the heldout forms.\n",
    "First, we require predicted vectors for the base forms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3957bfe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Shat_base = cue_obj_base.C * F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89976533",
   "metadata": {},
   "source": [
    "Next, we impute vectors for all features in the `:features` column in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = JudiLing.make_pS_matrix(train_small, features_col = :features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = JudiLing.make_transform_matrix(L.pS, S_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7796fd7",
   "metadata": {},
   "source": [
    "This way, we get a past tense vector, which we now add to the base vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e84a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "past_vec = W[L.f2i[\"past\"],:]\n",
    "\n",
    "S_base_past = Shat_base .+ past_vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6baa2d",
   "metadata": {},
   "source": [
    "Now, we first train the G matrix, and then predict Chat matrices for the training data, as well as for the heldout data based on the semantic vectors we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = JudiLing.make_transform_matrix(S_train, cue_obj_train.C);\n",
    "Chat = S_train * G;\n",
    "Chat_val_base_past = S_base_past * G;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431fe95",
   "metadata": {},
   "source": [
    "Now we run the learn paths algorithm on the heldout data. Note that we now pass the predicted C matrix based on the created semantic vectors (`Chat_val_base_past`) as well as those semantic vectors (`S_base_past`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093310f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_learn_base_past= JudiLing.learn_paths(\n",
    "train_small,\n",
    "val_small,\n",
    "cue_obj_train.C,\n",
    "S_base_past,\n",
    "F,\n",
    "Chat_val_base_past,\n",
    "cue_obj_train.A,\n",
    "cue_obj_train.i2f,\n",
    "cue_obj_train.f2i, # api changed in 0.3.1\n",
    "max_t = JudiLing.cal_max_timestep(val_small, :Word),\n",
    "max_can = 10,\n",
    "grams = 3,\n",
    "threshold = 0.01,\n",
    "is_tolerant=true,\n",
    "max_tolerance=1,\n",
    "tolerance=-1.,\n",
    "target_col = :Word,\n",
    "verbose = true,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d1d4e",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e869f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_acc(res_learn_base_past, cue_obj_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30367e58",
   "metadata": {},
   "source": [
    "Accuracy @10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f74b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_acc_loose(res_learn_base_past, cue_obj_val.gold_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25177fdd",
   "metadata": {},
   "source": [
    "Write to dataframe and join with validation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abeb274",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base_past = JudiLing.write2df(res_learn_base_past, val_small, cue_obj_train, cue_obj_val, target_col=:Word)\n",
    "df_base_past = leftjoin(df_base_past, val_small, on = :identifier => :Word)\n",
    "df_base_past[df_base_past.isbest .== true,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda48ec-6e7d-482e-9b9c-20db86b1e90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = df_base_past[df_base_past.isbest .== true,:]\n",
    "best[:, [\"identifier\", \"pred\", \"Regularity\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5b703",
   "metadata": {},
   "source": [
    "Compute accuracy for regular and irregular verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0161ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(groupby(df_base_past[df_base_past.isbest .== true,:], :Regularity), :iscorrect => mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d594f",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## Exercise 1: \n",
    "Rerun the second analysis (using past tense vectors created on the fly) using phonological representations. Note that you will have to create a new careful split. Use `random_seed = 42`. A dataframe with base forms can be found in `dat/english_heldout_base.csv`. How do the results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62762d1c",
   "metadata": {},
   "source": [
    "Split the data using random seed 42 and this time using `\"Phon\"` as the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77b4d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_phon, data_val_phon =\n",
    "JudiLing.loading_data_careful_split(\n",
    "\"../dat/english.csv\", \"english_phon\", \"../dat/careful\",\n",
    "[\"Lexeme\", \"Continuous\", \"Tense\", \"Person\", \"Number\"],\n",
    "n_grams_target_col = \"Phon\",\n",
    "grams = 3,\n",
    "val_sample_size = 300,\n",
    "random_seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac01403",
   "metadata": {},
   "source": [
    "Keep all past tense forms in the validation data and merge the rest back into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86caeda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_phon = vcat(data_train_phon, data_val_phon[data_val_phon.Tense .!= \"past\",:])\n",
    "data_val_phon = data_val_phon[data_val_phon.Tense .== \"past\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece30d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combine(groupby(data_val_phon, :Regularity), nrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f7aa8",
   "metadata": {},
   "source": [
    "Load the dataframe with baseforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95597c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_phon = DataFrame(CSV.File(\"../dat/english_heldout_base_phon2.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9698fd",
   "metadata": {},
   "source": [
    "Load semantic vectors for the words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581938a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_small_phon, S_train_phon = JudiLing.load_S_matrix_from_fasttext(data_train_phon, :en, target_col=:Word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c6828",
   "metadata": {},
   "source": [
    "Create cue matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d94290",
   "metadata": {},
   "outputs": [],
   "source": [
    "cue_obj_train_phon, cue_obj_base_phon = JudiLing.make_combined_cue_matrix(train_small_phon[:, [\"Phon\"]], base_phon[:, [\"Phon\"]], \n",
    " grams=3, target_col=\"Phon\")\n",
    "\n",
    "cue_obj_val_phon = JudiLing.make_cue_matrix(data_val_phon[:, [\"Phon\"]], cue_obj_train_phon,\n",
    " grams=3, target_col=\"Phon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66509bc0",
   "metadata": {},
   "source": [
    "Train F matrices, predict semantic matrix and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc396f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F_phon = JudiLing.make_transform_matrix(cue_obj_train_phon.C, S_train_phon)\n",
    "Shat_train_phon = cue_obj_train_phon.C * F_phon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a6c8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "JudiLing.eval_SC(Shat_train_phon, S_train_phon, train_small_phon, :Phon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd6404",
   "metadata": {},
   "source": [
    "Predict semantic vectors for the base forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d44bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Shat_base_phon = cue_obj_base_phon.C * F_phon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67b8d2",
   "metadata": {},
   "source": [
    "Create past tense form semantic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d0c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L_phon = JudiLing.make_pS_matrix(train_small_phon, features_col = :features);\n",
    "W_phon = JudiLing.make_transform_matrix(L_phon.pS, S_train_phon);\n",
    "past_vec_phon = W_phon[L_phon.f2i[\"past\"],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34c18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_base_past_phon = Shat_base_phon .+ past_vec_phon'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6113346",
   "metadata": {},
   "source": [
    "Train production matrix and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_phon = JudiLing.make_transform_matrix(S_train_phon, cue_obj_train_phon.C);\n",
    "Chat_phon = S_train_phon * G_phon;\n",
    "Chat_val_base_past_phon = S_base_past_phon * G_phon;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bb0ae",
   "metadata": {},
   "source": [
    "Run learn paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227cd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_learn_base_past_phon= JudiLing.learn_paths(\n",
    "train_small_phon,\n",
    "data_val_phon,\n",
    "cue_obj_train_phon.C,\n",
    "S_base_past_phon,\n",
    "F_phon,\n",
    "Chat_val_base_past_phon,\n",
    "cue_obj_train_phon.A,\n",
    "cue_obj_train_phon.i2f,\n",
    "cue_obj_train_phon.f2i, # api changed in 0.3.1\n",
    "max_t = JudiLing.cal_max_timestep(data_val_phon, :Phon),\n",
    "max_can = 10,\n",
    "grams = 3,\n",
    "threshold = 0.01,\n",
    "is_tolerant=true,\n",
    "max_tolerance=1,\n",
    "tolerance=-1.,\n",
    "target_col = :Phon,\n",
    "verbose = true,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc06918",
   "metadata": {},
   "source": [
    "Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91620987",
   "metadata": {},
   "outputs": [],
   "source": [
    "JudiLing.eval_acc(res_learn_base_past_phon, cue_obj_val_phon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e3833",
   "metadata": {},
   "source": [
    "Write to dataframe, join with full validation dataframe and display best supported candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15feae95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base_past_phon = JudiLing.write2df(res_learn_base_past_phon, data_val_phon, cue_obj_train_phon, cue_obj_val_phon, target_col=:Phon)\n",
    "df_base_past_phon = leftjoin(df_base_past_phon, data_val_phon, on = :identifier => :Phon)\n",
    "df_base_past_phon[df_base_past_phon.isbest .== true,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46336ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "last(df_base_past_phon[df_base_past_phon.isbest .== true,:],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50490dd",
   "metadata": {},
   "source": [
    "Accuracy for regulars and irregulars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af842b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(groupby(df_base_past_phon[df_base_past_phon.isbest .== true,:], :Regularity), :iscorrect => mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916ee60",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "In the case of past tense forms of phonological forms the model with past tense vectors created on the fly does not perform well (you can try running the very first analysis with past tense vectors from the embedding space as input; the results are similar). One possible reason for this drop in accuracy compared to orthographic representations could be that the regular orthographic representations are much more regular (always ending in \"ed\") compared to the phonological ones (sometimes ending in `d`, sometimes in `t`). This is particularly important when considering the trigram representation, where `ed#` forms one trigram while this is usually not the case for the phonological representation (try rerunning this analysis using biphones instead of triphones and you will see that the result improves quite dramatically. Just make sure you split the data still based on triphones, otherwise `english_heldout_base.csv` won't match the held-out data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7212f",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Rerun the first analysis (i.e. model using trigrams rather than triphones), but instead of making use of real past tense forms as the heldout data, use the following list of nonwords (from Albright & Hayes, 2003): bize, dize, flidge, fro, gare, glip, rife, stin, stip, blafe, bredge, chool, dape, gezz, nace, spack, stire, tesh, wiss, blig, chake, drit, fleep, gleed, glit, plim, queed, scride, spling, gude, nold, nung, pank, preak, rask, shilk, tark, teep, trisk, tunk.\n",
    "\n",
    "Is the model able to predict plausible past tense forms of the nonwords? What are problems the model has with these forms that it didn't have in the previous analyses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf849a",
   "metadata": {},
   "source": [
    "First, we create a dataframe with all the nonwords in a column called `:Word` to match the column name in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c9be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nonwords = \"bize, dize, flidge, fro, gare, glip, rife, stin, stip, blafe, bredge, chool, dape, gezz, nace, spack, stire, tesh, wiss, blig, chake, drit, fleep, gleed, glit, plim, queed, scride, spling, gude, nold, nung, pank, preak, rask, shilk, tark, teep, trisk, tunk\"\n",
    "nonwords = split(nonwords, \", \")\n",
    "nonwords_df = DataFrame(:Word => nonwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e25592",
   "metadata": {},
   "source": [
    "Now we create a dataframe for the training data and the nonwords. We can't create one for the heldout past tense forms because there are no \"correct\" past tense forms for these nonwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1788e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cue_obj_train_orth_nw, cue_obj_base_orth_nw = JudiLing.make_combined_cue_matrix(train_small[:, [\"Word\"]], nonwords_df[:, [\"Word\"]], \n",
    " grams=3, target_col=\"Word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8812",
   "metadata": {},
   "source": [
    "Train F matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67980d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F_orth_nw = JudiLing.make_transform_matrix(cue_obj_train_orth_nw.C, S_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b9d2b",
   "metadata": {},
   "source": [
    "Predict semantic vectors for the nonword base forms and add past tense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Shat_base_orth_nw = cue_obj_base_orth_nw.C * F_orth_nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc34f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S_base_past_orth_nw = Shat_base_orth_nw .+ past_vec'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a7098",
   "metadata": {},
   "source": [
    "Train G matrix and predict form vectors for nonword past tense forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02915e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_orth_nw = JudiLing.make_transform_matrix(S_train, cue_obj_train_orth_nw.C);\n",
    "Chat_val_base_past_orth_nw = S_base_past_orth_nw * G_orth_nw;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5a6e5",
   "metadata": {},
   "source": [
    "Run learn paths for the nonwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd679d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_learn_base_past_orth_nw= JudiLing.learn_paths(\n",
    "train_small,\n",
    "nonwords_df,\n",
    "cue_obj_train_orth_nw.C,\n",
    "S_base_past_orth_nw,\n",
    "F_orth_nw,\n",
    "Chat_val_base_past_orth_nw,\n",
    "cue_obj_train_orth_nw.A,\n",
    "cue_obj_train_orth_nw.i2f,\n",
    "cue_obj_train_orth_nw.f2i, # api changed in 0.3.1\n",
    "max_t = JudiLing.cal_max_timestep(nonwords_df, :Word),\n",
    "max_can = 10,\n",
    "grams = 3,\n",
    "threshold = 0.01,\n",
    "is_tolerant=true,\n",
    "max_tolerance=1,\n",
    "tolerance=-1.,\n",
    "target_col = :Word,\n",
    "verbose = true,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c600a57",
   "metadata": {},
   "source": [
    "Since there are no target forms for these nonwords, we can't compute any accuracy. Therefore we can only inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b39fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_base_past_orth_nw = JudiLing.write2df(res_learn_base_past_orth_nw, nonwords_df, cue_obj_train_orth_nw, cue_obj_base_orth_nw, target_col=:Word)\n",
    "df_base_past_orth_nw = leftjoin(df_base_past_orth_nw, nonwords_df, on = :identifier => :Word)\n",
    "first(df_base_past_orth_nw[df_base_past_orth_nw.isbest .== true,:], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f1352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last(df_base_past_orth_nw[df_base_past_orth_nw.isbest .== true,:], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c15d3",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "Overall, performance on these nonwords is somewhat worse than on the real words. Part of the reason is surely that some of the trigrams in the nonwords are not available in the training data, and that therefore the production model has a much harder task to solve. \n",
    "The most frequent \"error\" is again the \"no change\" error. There are a few instances where this might be a plausible past form such as for \"queed\" or \"gleed\". There are no plausible irregular-like stem changes. However, there are a few completely implausible forms, such as \"dinged\" for \"gude\" or \"crided\" for \"scride\". These are presumably due to missing trigrams as mentioned above. A good aspect of the produced forms is that they capture regularities such as the duplication of \"t\"s in words such as \"drit\" => \"dritted\" or \"glit\" => \"glitted\".\n",
    "\n",
    "Overall, the model seems to be able to deal with nonwords reasonably well, but it is also evident that performance will suffer if the words are phonologically or orthographically implausible (here the case if trigrams are missing). One way to improve this may be to use bigrams instead of trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997e089-1ef8-431a-a7aa-76f5fb23f18e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
